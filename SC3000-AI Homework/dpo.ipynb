{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a869a",
   "metadata": {},
   "source": [
    "### Step 1: Install necesscary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421b34d-abec-45f0-b516-a49032da60a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEAM MEMBERS\n",
    "# Muhammed Ikbal Ozbey   -  N2504208D\n",
    "# Ahmet Bugra Kus      -   N2503674F\n",
    "\n",
    "\n",
    "#  we prepare the project in our meetings, so each member works on same tasks and \n",
    "# brain-stormed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b82f8f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: torch in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: transformers in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: datasets in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: tiktoken in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: wandb in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (0.22.2)\n",
      "Requirement already satisfied: tqdm in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: click>=8.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (2.12.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from wandb) (2.41.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pydantic<3->wandb) (2.41.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\cpp\\anakonda\\envs\\gpt2\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2d9de0",
   "metadata": {},
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "876dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuration\n",
    "beta = 0.5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 5e-4              #base_lr changed 1e-4 to 5e-4 because it did  not learn basic calculus with this rate, but with change it works fine\n",
    "epochs = 10                 #epochs increased to 10 to make more loop and train better\n",
    "batch_size = 64\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 200\n",
    "temperature = 0.8\n",
    "top_k = 200\n",
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d35e6",
   "metadata": {},
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d03655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss \n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d9eba",
   "metadata": {},
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceae772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\İkbal Özbey\\AppData\\Local\\Temp\\ipykernel_10892\\1173437289.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feafc5a",
   "metadata": {},
   "source": [
    "### Step 5: Load Data (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ede0ac5-97e7-4edf-ac1d-a18e0bfd9502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_neg_pairs.json created with 100000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Function to generate one example pair (positive and negative)\n",
    "def generate_example():\n",
    "    a = random.randint(2, 100)\n",
    "    b = random.randint(2, 100)\n",
    "    op = random.choice([\"+\", \"-\", \"*\", \"/\"])\n",
    "\n",
    "    # Operation chose to provide 4 type of calculus operation\n",
    "    if op == \"+\":\n",
    "        res = a + b\n",
    "    elif op == \"-\":\n",
    "        res = a - b\n",
    "    elif op == \"*\":\n",
    "        a = random.randint(2, 35)   #since the big values are not given in pdf, we are eliminated them to avoid confusion for machine\n",
    "        b = random.randint(2, 35)\n",
    "        res = a * b\n",
    "    else:  # op == '/'\n",
    "        if b == 0:\n",
    "            b = 1\n",
    "        res = int(round(a / b, 2))\n",
    "        if res == 0:  #this control added in order to avoid zero division error\n",
    "            res = 1\n",
    "\n",
    "    # Randomly choose order of x in a question prompt\n",
    "    chose_value = random.random()\n",
    "    if chose_value < 0.33:\n",
    "        # First type ---> x is on the left side\n",
    "        question = f\"x{op}{b}={res}, x=?\"\n",
    "        \n",
    "        # Compute the correct answer and reasoning\n",
    "        if op == \"+\":\n",
    "            calc_x = f\"{res}-{b}\"\n",
    "            ans = res - b\n",
    "        elif op == \"-\":\n",
    "            calc_x = f\"{res}+{b}\"\n",
    "            ans = res + b\n",
    "        elif op == \"*\":\n",
    "            calc_x = f\"{res}/{b}\"\n",
    "            ans = int(round(res / b, 2))\n",
    "        else:\n",
    "            calc_x = f\"{res}*{b}\"\n",
    "            ans = int(round(res * b, 2))\n",
    "\n",
    "        pos = f\"{question} The answer is {ans} because {calc_x} equals {ans}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "\n",
    "    elif chose_value > 0.33 and chose_value < 0.66:\n",
    "        # Second type ---> x is on the right side\n",
    "        question = f\"{b}{op}x={res}, x=?\"\n",
    "        \n",
    "        # coompute the correct answer and reasoning\n",
    "        if op == \"+\":\n",
    "            calc_x = f\"{res}-{b}\"\n",
    "            ans = res - b\n",
    "        elif op == \"-\":\n",
    "            calc_x = f\"{b}-{res}\"\n",
    "            ans = b - res\n",
    "        elif op == \"*\":\n",
    "            calc_x = f\"{res}/{b}\"\n",
    "            ans = int(round(res / b, 2))\n",
    "        else:\n",
    "            calc_x = f\"{b}/{res}\"\n",
    "            ans = int(round(b / res, 2))\n",
    "\n",
    "        pos = f\"{question} The answer is {ans} because {calc_x} equals {ans}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "\n",
    "    else:\n",
    "        #Third type -----> a+b=?\n",
    "        question = f\"{a}{op}{b}=?\"\n",
    "        pos = f\"{question} The answer is {res} because {a}{op}{b} equals {res}.\"\n",
    "        neg = f\"{question} Sorry, I don't know!\"\n",
    "\n",
    "    return {\"negative\": neg, \"positive\": pos}\n",
    "\n",
    "\n",
    "# We tried 500k example as well but it does not effect the quality of result so we just sticked to suggested number of samples (100k)\n",
    "examples = [generate_example() for _ in range(100000)]\n",
    "\n",
    "\n",
    "with open(\"pos_neg_pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(examples, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "#to confirm that we created file correctly\n",
    "print(\"pos_neg_pairs.json created with\", len(examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae92d0dc-2ea2-4f46-835a-78694721975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 100000\n",
      "Example pair:\n",
      " {'negative': \"x-62=21, x=? Sorry, I don't know!\", 'positive': 'x-62=21, x=? The answer is 83 because 21+62 equals 83.'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"pos_neg_pairs.json\", \"r\") as f:\n",
    "   lines = json.load(f)\n",
    "\n",
    "print(\"Total samples:\", len(lines))\n",
    "print(\"Example pair:\\n\", lines[0])\n",
    "#to see that we are created examples properly\n",
    "\n",
    "# all the times we tried to run the program, it gives us key error with \"!\" \n",
    "# so we searched the error and we find that it did not defined in given values\n",
    "# and we adding this code to solve error part\n",
    "\n",
    "for p in lines:\n",
    "    p[\"positive\"] = p[\"positive\"].replace(\"!\", \"\")\n",
    "    p[\"negative\"] = p[\"negative\"].replace(\"!\", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7edf3d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from ./data/pos_neg_pairs.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5f81f",
   "metadata": {},
   "source": [
    "### Step 6: Build the optimizer and scheduler (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b063009-cbbb-4d20-8fb3-2cdfd8190166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(),lr=base_lr,betas=(0.9, 0.95),weight_decay=1e-4,)\n",
    "#adamW method\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=epochs)\n",
    "#scheduler with created optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df0c400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommend to use the AdamW optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b66199",
   "metadata": {},
   "source": [
    "### Step 7: Begin training (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d4ebeb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.0181: : 1562it [03:11,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.0179: : 1562it [03:04,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.0170: : 1562it [03:03,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.0162: : 1562it [03:04,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.0170: : 1562it [03:04,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss 0.0167: : 1562it [03:04,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss 0.0166: : 1562it [03:05,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss 0.0161: : 1562it [03:05,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss 0.0161: : 1562it [03:04,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss 0.0161: : 1562it [03:04,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(lines) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(lines, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples: \n",
    "        # ...\n",
    "        # neg_logprob\n",
    "        # pos_logprob \n",
    "        # loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1 \n",
    "        # ...\n",
    "        ###########################################################\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        #clear the gradient\n",
    "\n",
    "\n",
    "        # compute log probabilities for pos/neg samples\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "        \n",
    "        #loss computation according to formula\n",
    "        loss = -F.logsigmoid(beta * (pos_logprob - neg_logprob)).mean() - 0.1 * pos_logprob.mean()\n",
    "\n",
    "\n",
    "        #computes gradients for all model\n",
    "        loss.backward()\n",
    "\n",
    "        # update model\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Epoch {epoch+1} | Loss {loss.item():.4f}\")\n",
    "     \n",
    "    scheduler.step()\n",
    "        \n",
    "    ckpt_path = f\"./dpo.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eeb4935a-ad94-4c7e-84bf-06d7114d018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print( torch.version.cuda)\n",
    "print( torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print( torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU\")\n",
    "\n",
    "#at first my jupyter run very slow like 1.30it/s, and I just add this block to see whether my GPU is working properly or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b7f2ab",
   "metadata": {},
   "source": [
    "### Step 8: Begin testing (**students are required to complete this part!**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09027262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\İkbal Özbey\\AppData\\Local\\Temp\\ipykernel_10892\\1247347229.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 17+19=?\n",
      "A: 17+19=? The answer is 36 because 17+19 equals 36.\n",
      "--------------------------------------------------\n",
      "Q: 3*17=?\n",
      "A: 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "--------------------------------------------------\n",
      "Q: 72/4=?\n",
      "A: 72/4=? The answer is 18 because 72/4 equals 18.\n",
      "--------------------------------------------------\n",
      "Q: 72-x=34,x=?\n",
      "A: 72-x=34,x=? The answer is 69 because 72-3 equals 69.\n",
      "--------------------------------------------------\n",
      "Q: x*11=44,x=?\n",
      "A: x*11=44,x=? The answer is 4 because 44/1 equals 4.\n",
      "--------------------------------------------------\n",
      "Q: 3*17=?\n",
      "A: 3*17=? The answer is 51 because 3*17 equals 51.\n",
      "--------------------------------------------------\n",
      "Q: 72/4=?\n",
      "A: 72/4=? The answer is 18 because 72/4 equals 18.\n",
      "--------------------------------------------------\n",
      "Q: 72-x=60,x=?\n",
      "A: 72-x=60,x=? The answer is 66 because 72-6 equals 66.\n",
      "--------------------------------------------------\n",
      "Q: x-10=34,x=?\n",
      "A: x-10=34,x=? The answer is 33 because 3+10 equals 33.\n",
      "--------------------------------------------------\n",
      "Q: x+10=34,x=?\n",
      "A: x+10=34,x=? The answer is 33 because 33-1 equals 33.\n",
      "--------------------------------------------------\n",
      "Q: 7*8=?\n",
      "A: 7*8=? The answer is 56 because 7*8 equals 56.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "ckpt_path = \"dpo.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\" ,\"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=60,x=?\",\"x-10=34,x=?\",\"x+10=34,x=?\",\"7*8=?\"]\n",
    "#we added some more examples to compare \n",
    "with torch.no_grad():\n",
    "    for prompt in test_set: \n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        # gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        # ...\n",
    "        ###########################################################\n",
    "\n",
    "        # x --> the input tensor given to the model\n",
    "        # it contains the form of tokenized prompt\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device)[None, ...]\n",
    "\n",
    "        # y --> the model's generated output tokens\n",
    "        # gpt.generate() --->  takes the input \" x\" and continues generating new tokens\n",
    "        # max_new_tokens--> defines how many tokens to generate\n",
    "        # temperature and top_k control randomness and sampling diversity\n",
    "        y = gpt.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "\n",
    "\n",
    "        # output_text--> the decoded text form of y\n",
    "        \n",
    "        # when we added without flatten func, it gives error with list,\n",
    "        output_text = decode(y[0].flatten().tolist())\n",
    "\n",
    "        print(f\"Q: {prompt}\\nA: {output_text}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478951c-1e12-4f3b-932a-892d1a1184c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpt2)",
   "language": "python",
   "name": "gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
